---
title: 'Coursera Practical Machine Learning Course Project'
author: "Bill Cary"
date: "Sunday, February 22, 2015"
output:
  html_document:
    keep_md: yes
---

# Title - Weight Lifting Exercises Prediction
## Synopsis
The goal of this project is to predict the manner in which participants did the
exercise. This is the "classe" variable in the training set. Predictive models
may use any of the other variables. The final output is a report describing how
the model was constructed, how cross validation was used, the expected out of
sample error, and why various choices were made. The model will also be used to
predict 20 different test cases. 

## Model Building
This exercise is an example of a classification problem.  (As opposed to a
regression problem.)  Rather than predicting a continuous variable (regression),
we are predicting a categorical variable.  Specifically, we are attempting to
predict the result of a weight lifting exercise.  In this case, the result is
one of the following five classifications: exactly according to the
specification (Class A), throwing the elbows to the front (Class B), lifting the
dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D)
and throwing the hips to the front (Class E).

### Set defaults
Set knitr to echo code by default
```{r setoptions, echo = TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Set echo=TRUE by default
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(knitr)
opts_chunk$set(echo = TRUE)

```

### Prepare the environment
```{r prep}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Import required libraries
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
suppressMessages(library(ggplot2))     # General plotting functions
suppressMessages(library(plyr))        # Data manipulation
suppressMessages(library(dplyr))       # Data manipulation
suppressMessages(library(gridExtra))   # Grid layout for ggplot2 graphs
suppressMessages(library(scales))      # Axis scaling for ggplot2 graphs
suppressMessages(library(caret))       # Machine learning
suppressMessages(library(doParallel))  # Multicore processing

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Register parallel backend
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cl <- makeCluster(4)  # set appropriately for server on which job will run
registerDoParallel(cl)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Set paths for files and directory structure
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Get the local path
path <- getwd()

## Define other paths
path_code <- paste0(path, "/code/")
path_train <- paste0(path, '/data/pml-training.csv')
path_test <- paste0(path, '/data/pml-testing.csv')
path_results <- paste0(path, '/results/')

```

### Load the data
Read the raw datasets into R.  Because one dataset is large and the initial load
is time consuming, the cache=TRUE option is used to decrease the runtime after
the initial execution of the analysis.
```{r loaddata, cache=TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Read the data into R
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train <- read.csv(path_train, stringsAsFactors=FALSE,
                  na.strings = c('', 'NA', '#DIV/0!'))

test <- read.csv(path_test, stringsAsFactors=FALSE,
                 na.strings = c('', 'NA', '#DIV/0!'))

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Replace NA values with 0
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train[is.na(train)] <- 0
test[is.na(test)] <- 0

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Ensure classe variable is stored as Factor
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train$classe <- as.factor(train$classe)

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Drop features that will not be used for prediction
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train <- train[, 7:160]
test <- test[, 7:160]
```

### Partition the data
For this exercise, I chose to use 80% of the training records for actual model
building and training, and the remaining 20% for validation of the model.
```{r partition_train_data}
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Holdout 20% of training data for prelim testing/RMSLE estimates
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(107)
inTrain <- createDataPartition(y = train$classe,
                               p = 0.80,
                               list = FALSE)

training <- train[inTrain,]
testing <- train[-inTrain,]

```

### Build and Train the Model
I chose to utilize the Caret package to facilitate model building/training.
For this exercise, I have chosen to use _10-fold repeated cross validation_ with
_10 repeats_.

Because of the large number of features (159 features in addition to the classe
variable), and because I do not have any subject matter expertise regarding
these features, I have chosen to use a "deep learning" neural network model
(deepnet) that is capable of performing some degree of automated feature
selection.  However, prior to feeding data to the algorithm, I replace the NA
values in the dataset then then perform a Principle Components Analysis to
reduce the number of features considered by the model.  This preprocessing
is done using the preProcess function in the caret package.

In addition to the repeated cross validation, I use the Caret package (with a
tuning grid) to select the optimal tuning parameters for the network.  I allow
the algorithm to evaluate several alternative sizes for each of the three hidden
layers.

```{r buildmodel}
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Establish training control and tuning parameters
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Set up training control parameters (10-fold repeated cross validation)
fitControl <- trainControl(## 10-fold CV
        method = "repeatedcv",
        number = 10,
        ## repeated ten times
        repeats = 10,
        classProbs = TRUE)

# Set up the tuning grid as a three hidden layer network with various numbers of
# nodes in each hidden layer.
# grid <- expand.grid(layer1 = 5,
#                     layer2 = 3,
#                     layer3 = 3,
#                     hidden_dropout = 0,
#                     visible_dropout = 0)

grid <- expand.grid(mtry = c(5, 10, 15, 20))

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Drop columns with near zero variance (add no useful information for model)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nzv <- nearZeroVar(training)
training <- training[, -nzv]
testing <- testing[, -nzv]
test <- test[, -nzv]

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Preprocess the data (perform PCA)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
preprocValues <- preProcess(training[, -length(training)], method = c('pca'))

trainTransformed <- predict(preprocValues, training[, -length(training)])
testTransformed <- predict(preprocValues, testing[, -length(testing)])
validateTransformed <- predict(preprocValues, test[, -length(test)])

trainTransformedClasse <- training[, length(training)]
testTransformedClasse <- testing[, length(testing)]

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Train model
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(300)

model <- train(x = trainTransformed
               ,y = trainTransformedClasse
               ,data = trainTransformed
               ,method = 'rf'
               ,trControl = fitControl
               ,tuneGrid = grid
               ,verbose = FALSE
               ,metric = 'Accuracy')

## Print the Model and Model Summary        
print(model)
summary(model)

```



```{r cleandata2}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Standardize the PROPDMG and CROPDMG values according to the following rules, 
# based on the values in the PROPDMGEXP and CROPDMGEXP fields:
#
#        1) If PROPDMGEXP or CROPDMGEXP are blank, then leave the value as-is
#        2) If they equal 'H', then multiply by 100
#        3) If they equal 'K', then multiply by 1,000
#        4) If they equal 'M', then multiply by 1,000,000
#        5) If they equal 'B', then multiply by 1,000,000,000
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Create a function to handle the standardization
standardize <- function(x,y) {
        
        if (y == 'H'){
                x <- x * 100
                }
        else if (y == 'K'){
                x <- x * 1000
                }
        else if (y == 'M'){
                x <- x * 1000000
                }
        else if (y == 'B'){
                x <- x * 1000000000
                }
        else{
                x <- x
                }

return(x)

}

# Standardize the damage estimates
data$PROPDMG <- mapply(standardize,x=data$PROPDMG,y=data$PROPDMGEXP)
data$CROPDMG <- mapply(standardize,x=data$CROPDMG,y=data$CROPDMGEXP)

```


```{r healthhazard}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Add a column that summarizes injuries and fatalities
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
data$TOTALHAZARDS <- data$FATALITIES + data$INJURIES

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Summarize the data by Event Type using dplyr
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
sumdata <- group_by(data, EVTYPE) %>%
        summarise(sumfatalities = sum(FATALITIES, na.rm=TRUE),
                  suminjuries = sum(INJURIES, na.rm=TRUE),
                  sumhazards = sum(TOTALHAZARDS, na.rm=TRUE)) %>% 
        arrange(desc(sumhazards))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Select the top 10 event types by total hazards
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
top10health <- head(sumdata, 10)

top10health

```

```{r economicdamage}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Add a column that summarizes total economic damage caused by the event
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
data$TOTALDAMAGE <- data$PROPDMG + data$CROPDMG

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Summarize the data by Event Type using dplyr
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
sumdata <- group_by(data, EVTYPE) %>%
        summarise(sumpropdmg = sum(PROPDMG, na.rm=TRUE),
                  sumcropdmg = sum(CROPDMG, na.rm=TRUE),
                  sumtotaldmg = sum(TOTALDAMAGE, na.rm=TRUE)) %>% 
        arrange(desc(sumtotaldmg))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Select the top 10 event types by total economic damage
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
top10econ <- head(sumdata, 10)

top10econ
```

## Results

This analysis is being conducted for the purpose of answering two questions:

1. Across the United States, which types of events (as indicated in the EVTYPE
variable) are most harmful with respect to population health?
2. Across the United States, which types of events have the greatest economic
consequences?

For the purposes of answering these questions, I interpret the degree of harm of
an event with respect to population health as being the aggregate total of all
deaths and injuries (whether direct or indirect) caused by the event.  However,
I also make special note of the events which lead to the highest number of
direct and indirect deaths, as one can subjectively consider any injury, no
matter how severe, to be less severe than death.

I interpret economic damage for a given event as the sum of the property and
crop damage.  The primary objective of this part of the exercise is to identify
the greatest cause of economic damage, it is also useful to understand the
largest causes of each component of damage - crop and property damage.

*NOTE FOR GRADERS: The plot below is a single figure with multiple panels*
*constructed using the gridExtra package.*


```{r plots}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Plot the top 10 event types in descending order of magnitude
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
plot1 <- ggplot(data=top10health, aes(x=reorder(EVTYPE, -sumhazards),
                                      y=sumhazards)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('No of Occurences') +
        theme(text = element_text(size=8)) +
        ggtitle('Fatalities + Injuries') +
        scale_y_continuous(labels = comma)

plot2 <- ggplot(data=top10health, aes(x=reorder(EVTYPE, -sumfatalities),
                                      y=sumfatalities)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('No of Occurences') +
        theme(text = element_text(size=8)) +
        ggtitle('Fatalities') +
        scale_y_continuous(labels = comma)

plot3 <- ggplot(data=top10health, aes(x=reorder(EVTYPE, -suminjuries),
                                      y=suminjuries)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('No of Occurences') +
        theme(text = element_text(size=8)) +
        ggtitle('Injuries') +
        scale_y_continuous(labels = comma)

plot4 <- ggplot(data=top10econ, aes(x=reorder(EVTYPE, -sumtotaldmg),
                                    y=sumtotaldmg)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('$ Damages') +
        theme(text = element_text(size=8)) +
        ggtitle('Property + Crop') +
        scale_y_continuous(labels = comma)

plot5 <- ggplot(data=top10econ, aes(x=reorder(EVTYPE, -sumpropdmg),
                                    y=sumpropdmg)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('$ Damages') +
        theme(text = element_text(size=8)) +
        ggtitle('Property') +
        scale_y_continuous(labels = comma)

plot6 <- ggplot(data=top10econ, aes(x=reorder(EVTYPE, -sumcropdmg),
                                    y=sumcropdmg)) +
        geom_bar(stat="identity") +
        theme(axis.text.x=element_text(angle=45, hjust=1)) +
        xlab('') +
        ylab('$ Damages') +
        theme(text = element_text(size=8)) +
        ggtitle('Crop') +
        scale_y_continuous(labels = comma)

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow=2, ncol=3)

```

## Citation
Data for this analysis was provided by the Human Activity Recognition (HAR) project. The HAR website is located at http://groupware.les.inf.puc-rio.br/har. Please refer to the following citation for additional information on the dataset:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#sbia_paper_section#ixzz3QjclRIIW
