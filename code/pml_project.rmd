---
title: 'Coursera Practical Machine Learning Course Project'
author: "Bill Cary"
date: "Sunday, February 22, 2015"
output:
  html_document:
    keep_md: yes
---

# Title - Weight Lifting Exercises Prediction
## Synopsis
The goal of this project is to predict the manner in which participants did the
exercise. This is the "classe" variable in the training set. Predictive models
may use any of the other variables. The final output is a report describing how
the model was constructed, how cross validation was used, the expected out of
sample error, and why various choices were made. The model will also be used to
predict 20 different test cases. 

## Model Building
This exercise is an example of a classification problem.  (As opposed to a
regression problem.)  Rather than predicting a continuous variable (regression),
we are predicting a categorical variable.  Specifically, we are attempting to
predict the result of a weight lifting exercise.  In this case, the result is
one of the following five classifications: exactly according to the
specification (Class A), throwing the elbows to the front (Class B), lifting the
dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D)
and throwing the hips to the front (Class E).

### Set defaults
Set knitr to echo code by default
```{r setoptions, echo = TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Set echo=TRUE by default
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(knitr)
opts_chunk$set(echo = TRUE)

```

### Prepare the environment
```{r prep}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Import required libraries
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
suppressMessages(library(ggplot2))     # General plotting functions
suppressMessages(library(plyr))        # Data manipulation
suppressMessages(library(dplyr))       # Data manipulation
suppressMessages(library(gridExtra))   # Grid layout for ggplot2 graphs
suppressMessages(library(scales))      # Axis scaling for ggplot2 graphs
suppressMessages(library(caret))       # Machine learning
suppressMessages(library(doParallel))  # Multicore processing
suppressMessages(library(pROC))        # Model performance measurement

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Register parallel backend
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cl <- makeCluster(4)  # set appropriately for server on which job will run
registerDoParallel(cl)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Set paths for files and directory structure
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Get the local path
path <- getwd()

## Define other paths
path_code <- paste0(path, "/code/")
path_train <- paste0(path, '/data/pml-training.csv')
path_test <- paste0(path, '/data/pml-testing.csv')
path_results <- paste0(path, '/results/')

```

### Load the data
Read the raw datasets into R.  When reading the data into R, I chose to treat
all of the following strings as NA: '', 'NA', '#DIV/0!'.  I then replaced NA
values with zero (which I believe to be acceptable for this particular dataset),
and then trimmed off the first six columns of the data, as I did not intend to
use those columns for modeling.  I also ensure that the classe variable is
stored in R as a factor data type, to ensure that later modeling efforts are
conducted as classification exercises, rather than regression.

```{r loaddata, cache=TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Read the data into R
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train <- read.csv(path_train, stringsAsFactors=FALSE,
                  na.strings = c('', 'NA', '#DIV/0!'))

test <- read.csv(path_test, stringsAsFactors=FALSE,
                 na.strings = c('', 'NA', '#DIV/0!'))

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Replace NA values with 0
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train[is.na(train)] <- 0
test[is.na(test)] <- 0

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Ensure classe variable is stored as Factor
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train$classe <- as.factor(train$classe)

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Drop features that will not be used for prediction
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
train <- train[, 7:160]
test <- test[, 7:160]
```

### Partition the data
For this exercise, I chose to use 80% of the training records for actual model
building and training, and the remaining 20% for validation of the model.
```{r partition_train_data}
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Holdout 20% of training data for prelim testing/RMSLE estimates
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(107)
inTrain <- createDataPartition(y = train$classe,
                               p = 0.80,
                               list = FALSE)

training <- train[inTrain,]
testing <- train[-inTrain,]

```

### Data Cleansing

Before modeling, I performed a number of data cleansing/transformation steps to
simplify the input data and improve performance of the model.  The steps are
described below:

1. Remove columns with near-zero variance, as they will have little or no
predictive value.  I used the caret nearZeroVar function to identify the
columns in the training set having little or no variance.  I then removed these
columns from the training, testing and validation data sets.  This step removed
100 columns from the data sets.
2. Apply Principle Components Analysis (PCA) to the training set, then apply
the same PCA to the testing and validation sets.  This reduced the size of the
data sets from 54 columns down to 27 columns.

### Build and Train the Model
I chose to utilize the Caret package to facilitate model building/training.
For this exercise, I have chosen to use _10-fold repeated cross validation_ with
_10 repeats_.

I chose to model the data using a Random Forest algorithm (rf in R).  I utilized
the caret package to handle repeated cross validation and to select the optimal
tuning parameters (number of randomly chosen features in each tree).

The model results (within sample) are shown below:

```{r buildmodel}
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Establish training control and tuning parameters
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Set up training control parameters (10-fold repeated cross validation)
fitControl <- trainControl(## 10-fold CV
        method = "repeatedcv",
        number = 10,
        ## repeated ten times
        repeats = 10,
        classProbs = TRUE)

# Set up the tuning grid as a three hidden layer network with various numbers of
# nodes in each hidden layer.
# grid <- expand.grid(layer1 = 5,
#                     layer2 = 3,
#                     layer3 = 3,
#                     hidden_dropout = 0,
#                     visible_dropout = 0)

grid <- expand.grid(mtry = c(5, 10, 15, 20))

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Drop columns with near zero variance (add no useful information for model)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
nzv <- nearZeroVar(training)
training <- training[, -nzv]
testing <- testing[, -nzv]
test <- test[, -nzv]

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Preprocess the data (perform PCA)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
preprocValues <- preProcess(training[, -length(training)], method = c('pca'))

trainTransformed <- predict(preprocValues, training[, -length(training)])
testTransformed <- predict(preprocValues, testing[, -length(testing)])
validateTransformed <- predict(preprocValues, test[, -length(test)])

trainTransformedClasse <- training[, length(training)]
testTransformedClasse <- testing[, length(testing)]

## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Train model
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(300)

model <- train(x = trainTransformed
               ,y = trainTransformedClasse
               ,data = trainTransformed
               ,method = 'rf'
               ,trControl = fitControl
               ,tuneGrid = grid
               ,verbose = FALSE
               ,metric = 'Accuracy')

## Print the Model and Model Summary        
print(model)
summary(model)
confusionMatrix(model)

```

As can be seen from the figure below, the best cross-validation accuracy was
obtained by basing each tree on five randomly chosen features.

`r plot(model)`

### Evaluating Model Accuracy
Out-of-sample predictions (more indicative of true model performance against
unseen data) are shown below:
```{r accuracy}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Generate predictions against 20% of data held out for testing
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
test.predictions <- predict(model, testTransformed)
```

Confusion matrix:
`r confusionMatrix(test.predictions, testTransformedClasse)`

ROC curve plot:
`r roc <- multiclass.roc(as.numeric(testTransformedClasse), as.numeric(test.predictions), smooth = TRUE, plot = TRUE)`

As can be seen above, the performance of the model on out-of-sample data is
quite good.  This is also demonstrated by the results obtained when the model
was applied to the offficial test data and submitted for scoring; the correct
result was obtained for 19 of the 20 examples in the test set.


## Citation
Data for this analysis was provided by the Human Activity Recognition (HAR) project. The HAR website is located at http://groupware.les.inf.puc-rio.br/har. Please refer to the following citation for additional information on the dataset:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#sbia_paper_section#ixzz3QjclRIIW
